# Enhancing Large Language Models with Human Feedback
![alt text](assets/RLHF_picture.png)

```
    ├── assets                      # Illustrations for Report / README
    ├── RLHF.ipynb                  # Training of the Reward Model and PPO Optimization Notebook
    ├── generated-outputs-ppo.txt   # Generated outputs from the optimized language model       
    ├── README.md              
    └── REPORT.md                   # Comprehensive Report
```

Part of the M2 Course "Mathematics of Machine and Deep learning Algorithms".

## Overview 
This GitHub repository explores Transformers and incorporates Reinforcement Learning with Human Feedback (RLHF), focusing on its application to fine-tune Large Language Models (LLMs) using Proximal Policy Optimization (PPO).
This project demonstrates the effectiveness of combining Reinforcement Learning with Human Feedback and PPO to enhance Large Language Models. By aligning AI behavior with human preferences, this approach contributes to the development of more reliable and user-friendly AI systems.

## Methodology

- **Reward Model Training**: A reward model is trained to predict human preferences by evaluating the quality of responses generated by the language model (GPT-2). This model is fine-tuned using human feedback to accurately represent human preferences.

**Proximal Policy Optimization (PPO)**: Utilizing the trained reward model, PPO (a reinforcement learning algorithm) is employed to adjust the language model's policy. This optimization aligns the model's outputs with human preferences, enhancing its performance in generating desirable responses.

## Findings 

**Reward Model Training**: 

*RLHF steps*
![alt text](assets/RLHF_steps_read.png)


*Results with 1 epoch, 1242 training points*
- Training Loss = **0.82**
- Validation Loss = **0.69**
- Accuracy = **0.51**

**Proximal Policy Optimization (PPO)** : Architecture and generated outputs

![](assets/PPO_Stanford.png)

Mean Reward and KL divergence computed.

### Example of code to get generated output of the model: 

```python
# Step 1: Encode the query
query_txt = "This morning I went to the "  # Your input query
query_tensor = tokenizer.encode(query_txt, return_tensors="pt").to(model_with_ppo.device)

# Step 2: Define generation parameters
generation_kwargs = {
    "min_length": -1,
    "top_k": 0,        # Disable top-k sampling
    "top_p": 1.0,      # Enable nucleus sampling
    "do_sample": True, # Activate sampling
    "pad_token_id": tokenizer.eos_token_id,  # Padding token ID
    "max_new_tokens": 20,  # Maximum length of the generated response
}

# Step 3: Generate model response
response_tensor = model_with_ppo.generate(query_tensor, **generation_kwargs)

# Step 4: Decode the response
response_txt = tokenizer.decode(response_tensor[0])

# Display the generated response
print("Query:", query_txt)
print("Generated Response:", response_txt)

```

### Generated Output
"""
Query: This morning I went to the 
Generated Response: This morning I went to the  mile-long Air Base Towers where there are the buildings slightly above ground level and a trailer base
"""




